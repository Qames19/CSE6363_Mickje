{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id  vendor_id      pickup_datetime     dropoff_datetime  \\\n",
      "879655   id2425795          1  2016-01-08 23:55:11  2016-01-09 00:04:32   \n",
      "646838   id0767831          2  2016-03-05 09:52:06  2016-03-05 10:00:12   \n",
      "1138713  id0449104          1  2016-04-09 16:03:53  2016-04-09 16:21:22   \n",
      "864716   id3030157          1  2016-01-06 11:12:44  2016-01-06 11:19:49   \n",
      "434927   id1584885          1  2016-06-26 09:10:56  2016-06-26 09:17:44   \n",
      "\n",
      "         passenger_count  pickup_longitude  pickup_latitude  \\\n",
      "879655                 1        -73.955551        40.773346   \n",
      "646838                 1        -73.962181        40.763599   \n",
      "1138713                1        -73.977486        40.751842   \n",
      "864716                 1        -73.970001        40.762363   \n",
      "434927                 1        -73.950348        40.771561   \n",
      "\n",
      "         dropoff_longitude  dropoff_latitude store_and_fwd_flag  trip_duration  \n",
      "879655          -73.973640         40.763500                  N            561  \n",
      "646838          -73.980377         40.764919                  N            486  \n",
      "1138713         -74.011688         40.718925                  N           1049  \n",
      "864716          -73.963264         40.774666                  N            425  \n",
      "434927          -73.968178         40.762409                  N            408  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1312779 entries, 879655 to 121958\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count    Dtype  \n",
      "---  ------              --------------    -----  \n",
      " 0   id                  1312779 non-null  object \n",
      " 1   vendor_id           1312779 non-null  int64  \n",
      " 2   pickup_datetime     1312779 non-null  object \n",
      " 3   dropoff_datetime    1312779 non-null  object \n",
      " 4   passenger_count     1312779 non-null  int64  \n",
      " 5   pickup_longitude    1312779 non-null  float64\n",
      " 6   pickup_latitude     1312779 non-null  float64\n",
      " 7   dropoff_longitude   1312779 non-null  float64\n",
      " 8   dropoff_latitude    1312779 non-null  float64\n",
      " 9   store_and_fwd_flag  1312779 non-null  object \n",
      " 10  trip_duration       1312779 non-null  int64  \n",
      "dtypes: float64(4), int64(3), object(4)\n",
      "memory usage: 120.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "dataset = np.load(\"Datasets/nyc_taxi_data.npy\", allow_pickle=True).item()\n",
    "X_train, y_train, X_test, y_test = dataset[\"X_train\"], dataset[\"y_train\"], dataset[\"X_test\"], dataset[\"y_test\"]\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df_train = pd.DataFrame(X_train)\n",
    "df_train[\"trip_duration\"] = y_train  # Add target column\n",
    "\n",
    "print(df_train.head())  # View first few rows\n",
    "print(df_train.info())  # Check data types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keys are:  \n",
    "  - X_train  \n",
    "  - X_test  \n",
    "  - y_train  \n",
    "  - y_test  \n",
    "\n",
    "Extract our input_file into: X_train, y_train, X_test, y_test.\n",
    "\n",
    "Create a dataframe, df_train, of the X_train dataset for feature engineering and cleaning.  \n",
    "Add the target data, y_train, to df_train.\n",
    "Print the first 5 entries of df_train.  \n",
    "\n",
    "Features are:  \n",
    "  - id \\<obj\\>\n",
    "  - vendor_id \\<int64\\>\n",
    "  - pickup_datetime \\<obj\\>\n",
    "  - dropoff_datetime \\<obj\\>\n",
    "  - passenger_count \\<int64\\>\n",
    "  - pickup_longitude \\<float64\\>\n",
    "  - pickup_latitude \\<float64\\>\n",
    "  - dropoff_longitude \\<float64\\>\n",
    "  - dropoff_latitude \\<float64\\>\n",
    "  - store_and_fwd_flag \\<obj\\>\n",
    "  - trip_duration (test) \\<int64\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract time features for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Time Features\n",
    "# Convert to datetime\n",
    "df_train[\"pickup_datetime\"] = pd.to_datetime(df_train[\"pickup_datetime\"])\n",
    "df_train[\"dropoff_datetime\"] = pd.to_datetime(df_train[\"dropoff_datetime\"])\n",
    "\n",
    "# Extract Hour\n",
    "df_train[\"pickup_hour\"] = df_train[\"pickup_datetime\"].dt.hour\n",
    "df_train[\"dropoff_hour\"] = df_train[\"dropoff_datetime\"].dt.hour\n",
    "\n",
    "# Extract day of week\n",
    "df_train[\"pickup_day_of_week\"] = df_train[\"pickup_datetime\"].dt.weekday\n",
    "df_train[\"dropoff_day_of_week\"] = df_train[\"dropoff_datetime\"].dt.weekday\n",
    "\n",
    "# Extract month\n",
    "df_train[\"pickup_month\"] = df_train[\"pickup_datetime\"].dt.month\n",
    "df_train[\"dropoff_month\"] = df_train[\"dropoff_datetime\"].dt.month\n",
    "\n",
    "# Calculate sin/cos of pickup/dropoff hour to normalize values\n",
    "df_train[\"pickup_hour_sin\"] = np.sin(2 * np.pi * df_train[\"pickup_hour\"] /24)\n",
    "df_train[\"dropoff_hour_sin\"] = np.sin(2 * np.pi * df_train[\"dropoff_hour\"] /24)\n",
    "df_train[\"pickup_hour_cos\"] = np.cos(2 * np.pi * df_train[\"pickup_hour\"] /24)\n",
    "df_train[\"dropoff_hour_cos\"] = np.cos(2 * np.pi * df_train[\"dropoff_hour\"] /24)\n",
    "\n",
    "#Drop original timestamps (not required after feature extraction)\n",
    "df_train.drop(columns=[\"pickup_datetime\", \"dropoff_datetime\", \"pickup_hour\", \"dropoff_hour\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import MinMaxScaler to normalize the lat/long of pickup and dropoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Select the columns to normalize\n",
    "columns_to_normalize = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n",
    "\n",
    "# Apply Min-Max Scaling\n",
    "df_train[columns_to_normalize] = scaler.fit_transform(df_train[columns_to_normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove irrelevant columns \"id\" and \"vendor_id\".  It is possible that vendor_id may have some predictive power and that some vendors have quicker or slower trip times than others.  However, my intuition is that the location and time of day are more predictive of trip times than which company provides the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns=[\"id\", \"vendor_id\", \"store_and_fwd_flag\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.head())\n",
    "print(df_train.info())  # Ensure all columns are numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize target data with natural log (offset with 1 to avoid log 0)\n",
    "df_train[\"trip_duration\"] = np.log1p(df_train[\"trip_duration\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize normalization of trip_duration (target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot before and after transformation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Original Distribution - use np.expm1 as log inverse\n",
    "sns.histplot(np.expm1(df_train[\"trip_duration\"]), bins=50, kde=True, ax=axes[0])\n",
    "axes[0].set_title(\"Original Trip Duration Distribution\")\n",
    "\n",
    "# Log-Normalized Distribution\n",
    "sns.histplot(df_train[\"trip_duration\"], bins=50, kde=True, ax=axes[1])\n",
    "axes[1].set_title(\"Log-Normalized Trip Duration Distribution\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshoot crash due to memory\n",
    "import sys\n",
    "\n",
    "print(\"DataFrame size in memory (bytes):\", sys.getsizeof(df_train))\n",
    "print(\"Expected NumPy array size (bytes):\", df_train.memory_usage(deep=True).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check types for data to ensure not trying to convert <obj> types.\n",
    "print(df_train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now convert separately to avoid loading everything at once\n",
    "X_train = df_train.drop(columns=[\"trip_duration\"]).values.astype(np.float32)\n",
    "y_train = df_train[\"trip_duration\"].values.astype(np.float32)\n",
    "\n",
    "X_val = df_val.drop(columns=[\"trip_duration\"]).values.astype(np.float32)\n",
    "y_val = df_val[\"trip_duration\"].values.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define train_model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, epochs, batch_size, lr, patience, l2_reg, loss_function):\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            X_batch = X_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model.forward(X_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function.forward(y_batch, predictions)\n",
    "            \n",
    "            # Update loss\n",
    "            total_loss += loss\n",
    "\n",
    "            # Backward pass\n",
    "            grad_loss = loss_function.backward()\n",
    "            model.backward(grad_loss)\n",
    "\n",
    "            # Update weights\n",
    "            for layer in model.layers:\n",
    "                if hasattr(layer, \"weights\") and hasattr(layer, \"bias\"):\n",
    "                    np.clip(layer.grad_weights, -1, 1, out=layer.grad_weights)\n",
    "                    np.clip(layer.grad_bias, -1, 1, out=layer.grad_bias)\n",
    "\n",
    "                    layer.weights -= lr * (layer.grad_weights + l2_reg * layer.weights)\n",
    "                    layer.bias -= lr * layer.grad_bias\n",
    "\n",
    "        # Compute validation loss\n",
    "        val_predictions = model.forward(X_val)\n",
    "\n",
    "        val_loss = loss_function.forward(y_val, val_predictions)\n",
    "\n",
    "        training_losses.append(total_loss / len(X_train))\n",
    "        validation_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {training_losses[-1]:.6f}, Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered! Stopping training.\")\n",
    "            break\n",
    "\n",
    "    return training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define 3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Models\n",
    "from Layer_Implementations.Sequential import Sequential\n",
    "from Layer_Implementations.Linear import Linear\n",
    "from Layer_Implementations.Relu import Relu\n",
    "from Layer_Implementations.MeanSquaredError import MSELoss\n",
    "\n",
    "# Define input_size\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Define models\n",
    "model_1 = Sequential()\n",
    "model_1.add(Linear(input_size, 32))\n",
    "model_1.add(Relu())\n",
    "model_1.add(Linear(32, 1))\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(Linear(input_size, 32))\n",
    "model_2.add(Relu())\n",
    "model_2.add(Linear(32, 16))\n",
    "model_2.add(Relu())\n",
    "model_2.add(Linear(16, 1))\n",
    "\n",
    "model_3 = Sequential()\n",
    "model_3.add(Linear(input_size, 64))\n",
    "model_3.add(Relu())\n",
    "model_3.add(Linear(64, 32))\n",
    "model_3.add(Relu())\n",
    "model_3.add(Linear(32, 16))\n",
    "model_3.add(Relu())\n",
    "model_3.add(Linear(16, 1))\n",
    "\n",
    "models = [model_1, model_2, model_3]\n",
    "model_names = [\"Small Model\", \"Medium Model\", \"Large Model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters and run models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparmeters\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "patience = 3\n",
    "l2_reg = 0.01\n",
    "loss_function = MSELoss()\n",
    "\n",
    "# Initialize dictionary to store training_results\n",
    "training_results = {}\n",
    "\n",
    "# Loop through models and run each, storing values in training_results\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"\\nTraining {model_names[i]}:\\n\")\n",
    "\n",
    "    train_loss, val_loss = train_model(\n",
    "        model,\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        epochs, batch_size, learning_rate, patience, l2_reg,\n",
    "        loss_function\n",
    "    )\n",
    "\n",
    "    training_results[model_names[i]] = {\"train_loss\": train_loss, \"val_loss\": val_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Training Loss VS. Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for model_name, results in training_results.items():\n",
    "    plt.plot(results[\"val_loss\"], label=f\"{model_name} (Validation Loss)\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Validation Loss Comparison\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assign_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
